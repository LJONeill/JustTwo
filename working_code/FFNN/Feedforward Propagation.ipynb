{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f7a59b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from neural_networks import NeuralNet\n",
    "import scipy.special as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e57c977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data load\n",
    "#X_train = np.load('/Users/yasminebenmessaoud/Library/CloudStorage/OneDrive-ITU/Machine Learning/JustTwo/post_pca_data/X_train.npy')\n",
    "#Y_train = np.load('/Users/yasminebenmessaoud/Library/CloudStorage/OneDrive-ITU/Machine Learning/JustTwo/post_pca_data/Y_train.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7e47914",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('..\\\\..\\\\post_pca_data\\\\X_train_initial.npy')\n",
    "X_validate = np.load('..\\\\..\\\\post_pca_data\\\\X_validate_initial.npy')\n",
    "Y_train = np.load('..\\\\..\\\\post_pca_data\\\\Y_train_initial.npy')\n",
    "Y_validate = np.load('..\\\\..\\\\post_pca_data\\\\Y_validate_initial.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6fc1f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "#tried using this as the activation in hidden layers, and model got insignificantly worse (5 more incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2486f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(object):\n",
    "    RNG = np.random.default_rng()\n",
    "\n",
    "    def __init__(self, topology: list[int] = []):\n",
    "        self.topology = topology\n",
    "        self.weight_mats = []\n",
    "\n",
    "        self._init_matrices()\n",
    "\n",
    "    def _init_matrices(self):\n",
    "        # Set up weight matrices\n",
    "        if len(self.topology) > 1:\n",
    "            j = 1\n",
    "            for i in range(len(self.topology) - 1):\n",
    "                num_rows = self.topology[i]\n",
    "                num_cols = self.topology[j]\n",
    "\n",
    "                mat = self.RNG.random(size=(num_rows, num_cols))\n",
    "                self.weight_mats.append(mat)\n",
    "\n",
    "                j += 1\n",
    "\n",
    "    def feedforward(self, input_vector):\n",
    "        I = input_vector\n",
    "\n",
    "        for idx, W in enumerate(self.weight_mats):\n",
    "            I = np.dot(I, W)\n",
    "\n",
    "            if idx == len(self.weight_mats) - 1:\n",
    "                out_vector = sp.softmax(I)  # Output layer\n",
    "            else:\n",
    "                I = sp.softmax(I)  # Hidden layers\n",
    "\n",
    "        return out_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aea4a949",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define topology based on data's feature size and desired layers\n",
    "input_size = X_train.shape[1]  # Number of features in your data\n",
    "nnet = NeuralNet(topology=[input_size, 4, 5])  # Example topology: input -> 4 hidden -> 2 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5752bfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19161312 0.14907715 0.19715729 0.15914187 0.30301058]\n"
     ]
    }
   ],
   "source": [
    "# Perform feedforward on the first sample\n",
    "output = nnet.feedforward(X_train[500])  # Pass the first data point to the network\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ac01e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "def cross_entropy_loss(predictions, true_labels):\n",
    "    return -np.sum(true_labels * np.log(predictions + 1e-9)) / true_labels.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a133434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(neural_net, X_train, y_train, learning_rate, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for x, y_true in zip(X_train, y_train):\n",
    "            # Forward pass\n",
    "            y_pred = neural_net.feedforward(x)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = cross_entropy_loss(y_pred, y_true)\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            # Backpropagation (compute gradients) - this part needs to be implemented\n",
    "            \n",
    "            # Update weights - modify neural_net.weight_mats using the gradients\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(X_train)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e7930f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(neural_net, X_train, y_train, learning_rate, epochs):\n",
    "    def softmax_derivative(output):\n",
    "        # Softmax derivative for backpropagation\n",
    "        return output * (1 - output)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for x, y_true in zip(X_train, y_train):\n",
    "            # Forward pass\n",
    "            activations = [x]  # Store activations for backpropagation\n",
    "            current_input = x\n",
    "\n",
    "            # Forward propagate through layers\n",
    "            for W in neural_net.weight_mats:\n",
    "                current_input = sp.softmax(np.dot(current_input, W))  # Activation\n",
    "                activations.append(current_input)\n",
    "\n",
    "            y_pred = activations[-1]\n",
    "\n",
    "            # Compute loss\n",
    "            loss = cross_entropy_loss(y_pred, y_true)\n",
    "            epoch_loss += loss\n",
    "\n",
    "            # Backpropagation\n",
    "            deltas = []  # Store deltas for each layer\n",
    "            delta = y_pred - y_true  # Output layer delta\n",
    "            deltas.append(delta)\n",
    "\n",
    "            # Compute gradients for each layer (backpropagate)\n",
    "            for i in range(len(neural_net.weight_mats) - 1, 0, -1):\n",
    "                delta = (deltas[-1] @ neural_net.weight_mats[i].T) * softmax_derivative(activations[i])\n",
    "                deltas.append(delta)\n",
    "\n",
    "            deltas.reverse()  # Reverse deltas to match layers\n",
    "\n",
    "            # Update weights\n",
    "            for i in range(len(neural_net.weight_mats)):\n",
    "                grad = np.outer(activations[i], deltas[i])  # Gradient of the weights\n",
    "                neural_net.weight_mats[i] -= learning_rate * grad  # Gradient descent step\n",
    "        \n",
    "        # Print epoch loss\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(X_train)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e564ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_one_hot = encoder.fit_transform(Y_train.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99d8cf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.3216487703374046\n",
      "Epoch 2/100, Loss: 0.32234287216970126\n",
      "Epoch 3/100, Loss: 0.32234287202193257\n",
      "Epoch 4/100, Loss: 0.32234287202193257\n",
      "Epoch 5/100, Loss: 0.32234287202193257\n",
      "Epoch 6/100, Loss: 0.32234287202193257\n",
      "Epoch 7/100, Loss: 0.32234287202193257\n",
      "Epoch 8/100, Loss: 0.32234287202193257\n",
      "Epoch 9/100, Loss: 0.32234287202193257\n",
      "Epoch 10/100, Loss: 0.32234287202193257\n",
      "Epoch 11/100, Loss: 0.32234287202193257\n",
      "Epoch 12/100, Loss: 0.32234287202193257\n",
      "Epoch 13/100, Loss: 0.32234287202193257\n",
      "Epoch 14/100, Loss: 0.32234287202193257\n",
      "Epoch 15/100, Loss: 0.32234287202193257\n",
      "Epoch 16/100, Loss: 0.32234287202193257\n",
      "Epoch 17/100, Loss: 0.32234287202193257\n",
      "Epoch 18/100, Loss: 0.32234287202193257\n",
      "Epoch 19/100, Loss: 0.32234287202193257\n",
      "Epoch 20/100, Loss: 0.32234287202193257\n",
      "Epoch 21/100, Loss: 0.32234287202193257\n",
      "Epoch 22/100, Loss: 0.32234287202193257\n",
      "Epoch 23/100, Loss: 0.32234287202193257\n",
      "Epoch 24/100, Loss: 0.32234287202193257\n",
      "Epoch 25/100, Loss: 0.32234287202193257\n",
      "Epoch 26/100, Loss: 0.32234287202193257\n",
      "Epoch 27/100, Loss: 0.32234287202193257\n",
      "Epoch 28/100, Loss: 0.32234287202193257\n",
      "Epoch 29/100, Loss: 0.32234287202193257\n",
      "Epoch 30/100, Loss: 0.32234287202193257\n",
      "Epoch 31/100, Loss: 0.32234287202193257\n",
      "Epoch 32/100, Loss: 0.32234287202193257\n",
      "Epoch 33/100, Loss: 0.32234287202193257\n",
      "Epoch 34/100, Loss: 0.32234287202193257\n",
      "Epoch 35/100, Loss: 0.32234287202193257\n",
      "Epoch 36/100, Loss: 0.32234287202193257\n",
      "Epoch 37/100, Loss: 0.32234287202193257\n",
      "Epoch 38/100, Loss: 0.32234287202193257\n",
      "Epoch 39/100, Loss: 0.32234287202193257\n",
      "Epoch 40/100, Loss: 0.32234287202193257\n",
      "Epoch 41/100, Loss: 0.32234287202193257\n",
      "Epoch 42/100, Loss: 0.32234287202193257\n",
      "Epoch 43/100, Loss: 0.32234287202193257\n",
      "Epoch 44/100, Loss: 0.32234287202193257\n",
      "Epoch 45/100, Loss: 0.32234287202193257\n",
      "Epoch 46/100, Loss: 0.32234287202193257\n",
      "Epoch 47/100, Loss: 0.32234287202193257\n",
      "Epoch 48/100, Loss: 0.32234287202193257\n",
      "Epoch 49/100, Loss: 0.32234287202193257\n",
      "Epoch 50/100, Loss: 0.32234287202193257\n",
      "Epoch 51/100, Loss: 0.32234287202193257\n",
      "Epoch 52/100, Loss: 0.32234287202193257\n",
      "Epoch 53/100, Loss: 0.32234287202193257\n",
      "Epoch 54/100, Loss: 0.32234287202193257\n",
      "Epoch 55/100, Loss: 0.32234287202193257\n",
      "Epoch 56/100, Loss: 0.32234287202193257\n",
      "Epoch 57/100, Loss: 0.32234287202193257\n",
      "Epoch 58/100, Loss: 0.32234287202193257\n",
      "Epoch 59/100, Loss: 0.32234287202193257\n",
      "Epoch 60/100, Loss: 0.32234287202193257\n",
      "Epoch 61/100, Loss: 0.32234287202193257\n",
      "Epoch 62/100, Loss: 0.32234287202193257\n",
      "Epoch 63/100, Loss: 0.32234287202193257\n",
      "Epoch 64/100, Loss: 0.32234287202193257\n",
      "Epoch 65/100, Loss: 0.32234287202193257\n",
      "Epoch 66/100, Loss: 0.32234287202193257\n",
      "Epoch 67/100, Loss: 0.32234287202193257\n",
      "Epoch 68/100, Loss: 0.32234287202193257\n",
      "Epoch 69/100, Loss: 0.32234287202193257\n",
      "Epoch 70/100, Loss: 0.32234287202193257\n",
      "Epoch 71/100, Loss: 0.32234287202193257\n",
      "Epoch 72/100, Loss: 0.32234287202193257\n",
      "Epoch 73/100, Loss: 0.32234287202193257\n",
      "Epoch 74/100, Loss: 0.32234287202193257\n",
      "Epoch 75/100, Loss: 0.32234287202193257\n",
      "Epoch 76/100, Loss: 0.32234287202193257\n",
      "Epoch 77/100, Loss: 0.32234287202193257\n",
      "Epoch 78/100, Loss: 0.32234287202193257\n",
      "Epoch 79/100, Loss: 0.32234287202193257\n",
      "Epoch 80/100, Loss: 0.32234287202193257\n",
      "Epoch 81/100, Loss: 0.32234287202193257\n",
      "Epoch 82/100, Loss: 0.32234287202193257\n",
      "Epoch 83/100, Loss: 0.32234287202193257\n",
      "Epoch 84/100, Loss: 0.32234287202193257\n",
      "Epoch 85/100, Loss: 0.32234287202193257\n",
      "Epoch 86/100, Loss: 0.32234287202193257\n",
      "Epoch 87/100, Loss: 0.32234287202193257\n",
      "Epoch 88/100, Loss: 0.32234287202193257\n",
      "Epoch 89/100, Loss: 0.32234287202193257\n",
      "Epoch 90/100, Loss: 0.32234287202193257\n",
      "Epoch 91/100, Loss: 0.32234287202193257\n",
      "Epoch 92/100, Loss: 0.32234287202193257\n",
      "Epoch 93/100, Loss: 0.32234287202193257\n",
      "Epoch 94/100, Loss: 0.32234287202193257\n",
      "Epoch 95/100, Loss: 0.32234287202193257\n",
      "Epoch 96/100, Loss: 0.32234287202193257\n",
      "Epoch 97/100, Loss: 0.32234287202193257\n",
      "Epoch 98/100, Loss: 0.32234287202193257\n",
      "Epoch 99/100, Loss: 0.32234287202193257\n",
      "Epoch 100/100, Loss: 0.32234287202193257\n"
     ]
    }
   ],
   "source": [
    "# Define training parameters\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "# Train the model\n",
    "train(nnet, X_train, y_train_one_hot, learning_rate, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a26efa83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 4\n"
     ]
    }
   ],
   "source": [
    "# Predict for a new data sample (e.g., the first row of your dataset)\n",
    "new_sample = X_train[0]\n",
    "prediction = nnet.feedforward(new_sample)\n",
    "\n",
    "# Convert the output (softmax probabilities) to a predicted class\n",
    "predicted_class = np.argmax(prediction)\n",
    "print(f\"Predicted Class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4857aecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.32258114539473903\n",
      "Epoch 2/100, Loss: 0.32209372076637915\n",
      "Epoch 3/100, Loss: 0.32234266377109116\n",
      "Epoch 4/100, Loss: 0.32234234124340616\n",
      "Epoch 5/100, Loss: 0.3223421438592987\n",
      "Epoch 6/100, Loss: 0.32234194728500426\n",
      "Epoch 7/100, Loss: 0.3223417515218698\n",
      "Epoch 8/100, Loss: 0.3223415565712005\n",
      "Epoch 9/100, Loss: 0.32234136243424844\n",
      "Epoch 10/100, Loss: 0.32234116911221206\n",
      "Epoch 11/100, Loss: 0.3223409766062362\n",
      "Epoch 12/100, Loss: 0.3223407849174117\n",
      "Epoch 13/100, Loss: 0.32234059404677523\n",
      "Epoch 14/100, Loss: 0.32234040399530933\n",
      "Epoch 15/100, Loss: 0.322340214763942\n",
      "Epoch 16/100, Loss: 0.32234002635354647\n",
      "Epoch 17/100, Loss: 0.32233983876494154\n",
      "Epoch 18/100, Loss: 0.32233965199889103\n",
      "Epoch 19/100, Loss: 0.32233946605610375\n",
      "Epoch 20/100, Loss: 0.32233928093723363\n",
      "Epoch 21/100, Loss: 0.32233909664287946\n",
      "Epoch 22/100, Loss: 0.3223389131735849\n",
      "Epoch 23/100, Loss: 0.3223387305298384\n",
      "Epoch 24/100, Loss: 0.32233854871207346\n",
      "Epoch 25/100, Loss: 0.32233836772066815\n",
      "Epoch 26/100, Loss: 0.3223381875559454\n",
      "Epoch 27/100, Loss: 0.32233800821817327\n",
      "Epoch 28/100, Loss: 0.3223378297075645\n",
      "Epoch 29/100, Loss: 0.32233765202427694\n",
      "Epoch 30/100, Loss: 0.32233747516841343\n",
      "Epoch 31/100, Loss: 0.32233729914002207\n",
      "Epoch 32/100, Loss: 0.32233712393909614\n",
      "Epoch 33/100, Loss: 0.3223369495655744\n",
      "Epoch 34/100, Loss: 0.32233677601934113\n",
      "Epoch 35/100, Loss: 0.32233660330022634\n",
      "Epoch 36/100, Loss: 0.32233643140800583\n",
      "Epoch 37/100, Loss: 0.3223362603424016\n",
      "Epoch 38/100, Loss: 0.32233609010308184\n",
      "Epoch 39/100, Loss: 0.3223359206896612\n",
      "Epoch 40/100, Loss: 0.3223357521017011\n",
      "Epoch 41/100, Loss: 0.32233558433870996\n",
      "Epoch 42/100, Loss: 0.3223354174001432\n",
      "Epoch 43/100, Loss: 0.3223352512854041\n",
      "Epoch 44/100, Loss: 0.32233508599384336\n",
      "Epoch 45/100, Loss: 0.3223349215247599\n",
      "Epoch 46/100, Loss: 0.3223347578774012\n",
      "Epoch 47/100, Loss: 0.3223345950509632\n",
      "Epoch 48/100, Loss: 0.32233443304459103\n",
      "Epoch 49/100, Loss: 0.3223342718573793\n",
      "Epoch 50/100, Loss: 0.32233411148837227\n",
      "Epoch 51/100, Loss: 0.32233395193656444\n",
      "Epoch 52/100, Loss: 0.32233379320090083\n",
      "Epoch 53/100, Loss: 0.3223336352802775\n",
      "Epoch 54/100, Loss: 0.32233347817354163\n",
      "Epoch 55/100, Loss: 0.32233332187949243\n",
      "Epoch 56/100, Loss: 0.3223331663968812\n",
      "Epoch 57/100, Loss: 0.32233301172441203\n",
      "Epoch 58/100, Loss: 0.3223328578607419\n",
      "Epoch 59/100, Loss: 0.32233270480448156\n",
      "Epoch 60/100, Loss: 0.32233255255419585\n",
      "Epoch 61/100, Loss: 0.322332401108404\n",
      "Epoch 62/100, Loss: 0.3223322504655803\n",
      "Epoch 63/100, Loss: 0.32233210062415457\n",
      "Epoch 64/100, Loss: 0.3223319515825129\n",
      "Epoch 65/100, Loss: 0.3223318033389976\n",
      "Epoch 66/100, Loss: 0.3223316558919082\n",
      "Epoch 67/100, Loss: 0.32233150923950205\n",
      "Epoch 68/100, Loss: 0.32233136337999435\n",
      "Epoch 69/100, Loss: 0.32233121831155925\n",
      "Epoch 70/100, Loss: 0.32233107403233\n",
      "Epoch 71/100, Loss: 0.3223309305403999\n",
      "Epoch 72/100, Loss: 0.32233078783382235\n",
      "Epoch 73/100, Loss: 0.32233064591061195\n",
      "Epoch 74/100, Loss: 0.32233050476874486\n",
      "Epoch 75/100, Loss: 0.32233036440615914\n",
      "Epoch 76/100, Loss: 0.32233022482075585\n",
      "Epoch 77/100, Loss: 0.3223300860103992\n",
      "Epoch 78/100, Loss: 0.3223299479729175\n",
      "Epoch 79/100, Loss: 0.3223298107061032\n",
      "Epoch 80/100, Loss: 0.32232967420771436\n",
      "Epoch 81/100, Loss: 0.32232953847547435\n",
      "Epoch 82/100, Loss: 0.3223294035070732\n",
      "Epoch 83/100, Loss: 0.32232926930016764\n",
      "Epoch 84/100, Loss: 0.32232913585238226\n",
      "Epoch 85/100, Loss: 0.3223290031613096\n",
      "Epoch 86/100, Loss: 0.3223288712245111\n",
      "Epoch 87/100, Loss: 0.3223287400395176\n",
      "Epoch 88/100, Loss: 0.32232860960383025\n",
      "Epoch 89/100, Loss: 0.3223284799149206\n",
      "Epoch 90/100, Loss: 0.3223283509702316\n",
      "Epoch 91/100, Loss: 0.3223282227671782\n",
      "Epoch 92/100, Loss: 0.3223280953031478\n",
      "Epoch 93/100, Loss: 0.3223279685755013\n",
      "Epoch 94/100, Loss: 0.32232784258157304\n",
      "Epoch 95/100, Loss: 0.3223277173186721\n",
      "Epoch 96/100, Loss: 0.32232759278408246\n",
      "Epoch 97/100, Loss: 0.32232746897506404\n",
      "Epoch 98/100, Loss: 0.32232734588885287\n",
      "Epoch 99/100, Loss: 0.32232722352266197\n",
      "Epoch 100/100, Loss: 0.32232710187368196\n",
      "Predicted Class: 4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.special as sp\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# Neural Network Class\n",
    "class NeuralNet:\n",
    "    RNG = np.random.default_rng()\n",
    "\n",
    "    def __init__(self, topology: list[int]):\n",
    "        self.topology = topology\n",
    "        self.weight_mats = []\n",
    "        self._init_matrices()\n",
    "\n",
    "    def _init_matrices(self):\n",
    "        for i in range(len(self.topology) - 1):\n",
    "            num_rows = self.topology[i]\n",
    "            num_cols = self.topology[i + 1]\n",
    "            mat = self.RNG.random(size=(num_rows, num_cols))\n",
    "            self.weight_mats.append(mat)\n",
    "\n",
    "    def feedforward(self, input_vector):\n",
    "        I = input_vector\n",
    "        for idx, W in enumerate(self.weight_mats):\n",
    "            I = np.dot(I, W)\n",
    "            I = sp.softmax(I)\n",
    "        return I\n",
    "\n",
    "\n",
    "# Cross-Entropy Loss Function\n",
    "def cross_entropy_loss(predictions, true_labels):\n",
    "    return -np.sum(true_labels * np.log(predictions + 1e-9)) / predictions.shape[0]\n",
    "\n",
    "\n",
    "# Training Function\n",
    "def train(neural_net, X_train, y_train, learning_rate, epochs):\n",
    "    def softmax_derivative(output):\n",
    "        return output * (1 - output)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for x, y_true in zip(X_train, y_train):\n",
    "            # Forward pass\n",
    "            activations = [x]\n",
    "            current_input = x\n",
    "            for W in neural_net.weight_mats:\n",
    "                current_input = sp.softmax(np.dot(current_input, W))\n",
    "                activations.append(current_input)\n",
    "            y_pred = activations[-1]\n",
    "\n",
    "            # Compute loss\n",
    "            loss = cross_entropy_loss(y_pred, y_true)\n",
    "            epoch_loss += loss\n",
    "\n",
    "            # Backpropagation\n",
    "            deltas = [y_pred - y_true]  # Output layer delta\n",
    "            for i in range(len(neural_net.weight_mats) - 1, 0, -1):\n",
    "                delta = (deltas[-1] @ neural_net.weight_mats[i].T) * softmax_derivative(activations[i])\n",
    "                deltas.append(delta)\n",
    "            deltas.reverse()\n",
    "\n",
    "            # Update weights\n",
    "            for i in range(len(neural_net.weight_mats)):\n",
    "                grad = np.outer(activations[i], deltas[i])\n",
    "                neural_net.weight_mats[i] -= learning_rate * grad\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(X_train)}\")\n",
    "\n",
    "\n",
    "# Main Code\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    X_train = np.load('..\\\\..\\\\post_pca_data\\\\X_train_initial.npy')\n",
    "    X_validate = np.load('..\\\\..\\\\post_pca_data\\\\X_validate_initial.npy')\n",
    "    Y_train = np.load('..\\\\..\\\\post_pca_data\\\\Y_train_initial.npy')\n",
    "    Y_validate = np.load('..\\\\..\\\\post_pca_data\\\\Y_validate_initial.npy')\n",
    "\n",
    "    # One-hot encode labels\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    y_train_one_hot = encoder.fit_transform(Y_train.reshape(-1, 1))\n",
    "\n",
    "    # Define topology and initialize the neural network\n",
    "    input_size = X_train.shape[1]\n",
    "    output_size = y_train_one_hot.shape[1]\n",
    "    nnet = NeuralNet(topology=[input_size, 4, output_size])\n",
    "\n",
    "    # Train the network\n",
    "    learning_rate = 0.01\n",
    "    epochs = 100\n",
    "    train(nnet, X_train, y_train_one_hot, learning_rate, epochs)\n",
    "\n",
    "    # Predict for a new data sample\n",
    "    new_sample = X_train[0]\n",
    "    prediction = nnet.feedforward(new_sample)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    print(f\"Predicted Class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecb0e428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.32236830157558916\n",
      "Epoch 2/100, Loss: 0.3223074503760843\n",
      "Epoch 3/100, Loss: 0.3223074412415385\n",
      "Epoch 4/100, Loss: 0.3223074321407\n",
      "Epoch 5/100, Loss: 0.32230742307202176\n",
      "Epoch 6/100, Loss: 0.3223074140353452\n",
      "Epoch 7/100, Loss: 0.32230740503051253\n",
      "Epoch 8/100, Loss: 0.3223073960573671\n",
      "Epoch 9/100, Loss: 0.32230738711575324\n",
      "Epoch 10/100, Loss: 0.32230737820551614\n",
      "Epoch 11/100, Loss: 0.32230736932650206\n",
      "Epoch 12/100, Loss: 0.32230736047855807\n",
      "Epoch 13/100, Loss: 0.3223073516615323\n",
      "Epoch 14/100, Loss: 0.3223073428752737\n",
      "Epoch 15/100, Loss: 0.3223073341196322\n",
      "Epoch 16/100, Loss: 0.3223073253944587\n",
      "Epoch 17/100, Loss: 0.322307316699605\n",
      "Epoch 18/100, Loss: 0.3223073080349237\n",
      "Epoch 19/100, Loss: 0.32230729940026825\n",
      "Epoch 20/100, Loss: 0.3223072907954933\n",
      "Epoch 21/100, Loss: 0.3223072822204541\n",
      "Epoch 22/100, Loss: 0.32230727367500683\n",
      "Epoch 23/100, Loss: 0.3223072651590087\n",
      "Epoch 24/100, Loss: 0.3223072566723175\n",
      "Epoch 25/100, Loss: 0.3223072482147922\n",
      "Epoch 26/100, Loss: 0.3223072397862924\n",
      "Epoch 27/100, Loss: 0.3223072313866785\n",
      "Epoch 28/100, Loss: 0.32230722301581205\n",
      "Epoch 29/100, Loss: 0.3223072146735551\n",
      "Epoch 30/100, Loss: 0.32230720635977084\n",
      "Epoch 31/100, Loss: 0.3223071980743229\n",
      "Epoch 32/100, Loss: 0.3223071898170761\n",
      "Epoch 33/100, Loss: 0.32230718158789584\n",
      "Epoch 34/100, Loss: 0.32230717338664844\n",
      "Epoch 35/100, Loss: 0.32230716521320096\n",
      "Epoch 36/100, Loss: 0.3223071570674213\n",
      "Epoch 37/100, Loss: 0.32230714894917817\n",
      "Epoch 38/100, Loss: 0.3223071408583409\n",
      "Epoch 39/100, Loss: 0.3223071327947798\n",
      "Epoch 40/100, Loss: 0.32230712475836587\n",
      "Epoch 41/100, Loss: 0.3223071167489709\n",
      "Epoch 42/100, Loss: 0.32230710876646734\n",
      "Epoch 43/100, Loss: 0.32230710081072855\n",
      "Epoch 44/100, Loss: 0.32230709288162857\n",
      "Epoch 45/100, Loss: 0.3223070849790422\n",
      "Epoch 46/100, Loss: 0.3223070771028449\n",
      "Epoch 47/100, Loss: 0.32230706925291297\n",
      "Epoch 48/100, Loss: 0.3223070614291234\n",
      "Epoch 49/100, Loss: 0.3223070536313539\n",
      "Epoch 50/100, Loss: 0.32230704585948294\n",
      "Epoch 51/100, Loss: 0.3223070381133896\n",
      "Epoch 52/100, Loss: 0.3223070303929539\n",
      "Epoch 53/100, Loss: 0.3223070226980562\n",
      "Epoch 54/100, Loss: 0.32230701502857795\n",
      "Epoch 55/100, Loss: 0.322307007384401\n",
      "Epoch 56/100, Loss: 0.32230699976540805\n",
      "Epoch 57/100, Loss: 0.32230699217148245\n",
      "Epoch 58/100, Loss: 0.32230698460250823\n",
      "Epoch 59/100, Loss: 0.3223069770583701\n",
      "Epoch 60/100, Loss: 0.3223069695389534\n",
      "Epoch 61/100, Loss: 0.32230696204414416\n",
      "Epoch 62/100, Loss: 0.322306954573829\n",
      "Epoch 63/100, Loss: 0.32230694712789554\n",
      "Epoch 64/100, Loss: 0.3223069397062316\n",
      "Epoch 65/100, Loss: 0.32230693230872576\n",
      "Epoch 66/100, Loss: 0.3223069249352675\n",
      "Epoch 67/100, Loss: 0.3223069175857468\n",
      "Epoch 68/100, Loss: 0.3223069102600541\n",
      "Epoch 69/100, Loss: 0.3223069029580807\n",
      "Epoch 70/100, Loss: 0.3223068956797183\n",
      "Epoch 71/100, Loss: 0.3223068884248596\n",
      "Epoch 72/100, Loss: 0.3223068811933975\n",
      "Epoch 73/100, Loss: 0.3223068739852258\n",
      "Epoch 74/100, Loss: 0.32230686680023873\n",
      "Epoch 75/100, Loss: 0.32230685963833117\n",
      "Epoch 76/100, Loss: 0.3223068524993988\n",
      "Epoch 77/100, Loss: 0.3223068453833376\n",
      "Epoch 78/100, Loss: 0.3223068382900443\n",
      "Epoch 79/100, Loss: 0.3223068312194162\n",
      "Epoch 80/100, Loss: 0.3223068241713512\n",
      "Epoch 81/100, Loss: 0.3223068171457478\n",
      "Epoch 82/100, Loss: 0.3223068101425049\n",
      "Epoch 83/100, Loss: 0.32230680316152227\n",
      "Epoch 84/100, Loss: 0.3223067962027001\n",
      "Epoch 85/100, Loss: 0.322306789265939\n",
      "Epoch 86/100, Loss: 0.32230678235114035\n",
      "Epoch 87/100, Loss: 0.322306775458206\n",
      "Epoch 88/100, Loss: 0.32230676858703844\n",
      "Epoch 89/100, Loss: 0.32230676173754064\n",
      "Epoch 90/100, Loss: 0.322306754909616\n",
      "Epoch 91/100, Loss: 0.3223067481031688\n",
      "Epoch 92/100, Loss: 0.3223067413181035\n",
      "Epoch 93/100, Loss: 0.32230673455432524\n",
      "Epoch 94/100, Loss: 0.3223067278117398\n",
      "Epoch 95/100, Loss: 0.32230672109025327\n",
      "Epoch 96/100, Loss: 0.32230671438977254\n",
      "Epoch 97/100, Loss: 0.32230670771020464\n",
      "Epoch 98/100, Loss: 0.32230670105145753\n",
      "Epoch 99/100, Loss: 0.3223066944134394\n",
      "Epoch 100/100, Loss: 0.32230668779605914\n",
      "Predicted Class: 4\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "train(nnet, X_train, y_train_one_hot, learning_rate, epochs)\n",
    "\n",
    "# Predict for a new data sample\n",
    "new_sample = X_train[56]\n",
    "prediction = nnet.feedforward(new_sample)\n",
    "predicted_class = np.argmax(prediction)\n",
    "print(f\"Predicted Class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60e0019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for saving/loading the model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abc5311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "filename = 'our_FFNN_model.sav'\n",
    "\n",
    "pickle.dump(nnet, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d569b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "saved_model = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa79cbea",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1336af4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct prediction count\n",
      "403\n",
      "Correct prediction ratio\n",
      "0.2015\n"
     ]
    }
   ],
   "source": [
    "# Predictions for X_validate\n",
    "\n",
    "validation_predictions = []\n",
    "\n",
    "for datapoint in X_validate:\n",
    "    prediction = nnet.feedforward(datapoint)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    validation_predictions.append(predicted_class)\n",
    "\n",
    "print('Correct prediction count')\n",
    "print(sum(validation_predictions == Y_validate))\n",
    "\n",
    "print('Correct prediction ratio')\n",
    "print(sum(validation_predictions == Y_validate)/len(Y_validate))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
