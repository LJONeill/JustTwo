{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree implementation\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from treenode import TreeNode\n",
    "\n",
    "class DecisionTree:\n",
    "    \n",
    "    def __init__(self, max_depth, min_samples_leaf, min_information_gain, numb_of_features_splitting):\n",
    "        \"\"\"Initialize the decision tree with specified parameters.\"\"\"\n",
    "        self.max_depth = max_depth \n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_information_gain = min_information_gain\n",
    "        self.numb_of_features_splitting = numb_of_features_splitting\n",
    "\n",
    "    def _entropy(self, probability):\n",
    "        \"\"\"Calculate entropy based on a list of probabilities.\"\"\"\n",
    "        entropy_sum = 0\n",
    "        for p in probability:\n",
    "            if p > 0:\n",
    "                entropy_sum += -p * np.log2(p)\n",
    "        return entropy_sum\n",
    "\n",
    "    def _class_probabilities(self, categories):\n",
    "        \"\"\"Calculate the probabilities of each category.\"\"\"\n",
    "        total_count = len(categories)\n",
    "        category_counts = Counter(categories)\n",
    "        probabilities = []\n",
    "        for count in category_counts.values():\n",
    "            probabilities.append(count / total_count)\n",
    "        return probabilities\n",
    "\n",
    "    def _data_entropy(self, categories: list) -> float:\n",
    "        \"\"\"Calculate entropy for a list of categories.\"\"\"\n",
    "        class_probs = self._class_probabilities(categories)\n",
    "        data_entropy = self._entropy(class_probs)\n",
    "        return data_entropy\n",
    "    \n",
    "    def _partition_entropy(self, subsets: list) -> float:\n",
    "        \"\"\"Calculate the weighted entropy for a list of subsets of target values.\"\"\"\n",
    "        total_count = 0\n",
    "        for subset in subsets:\n",
    "            subset_length = len(subset)\n",
    "            total_count += subset_length  # Add each subset's length to total_count\n",
    "\n",
    "        weighted_entropy = 0\n",
    "        for subset in subsets:\n",
    "            subset_entropy = self._data_entropy(subset)\n",
    "            subset_weight = len(subset) / total_count\n",
    "            weighted_entropy += subset_entropy * subset_weight\n",
    "        return weighted_entropy\n",
    "\n",
    "    def _split(self, data: np.array, feature_idx: int, feature_val: float) -> tuple:\n",
    "        \"\"\"Split the data into two subsets based on a feature threshold.\"\"\"\n",
    "        mask = data[:, feature_idx] < feature_val\n",
    "        subset1 = data[mask]\n",
    "        subset2 = data[~mask]\n",
    "        return subset1, subset2\n",
    "\n",
    "    def _select_features_to_use(self, data: np.array) -> list:\n",
    "        \"\"\"Select features to consider for splitting.\"\"\"\n",
    "        total_features = list(range(data.shape[1] - 1))\n",
    "        num_features = len(total_features)\n",
    "\n",
    "        if self.numb_of_features_splitting == \"sqrt\":\n",
    "            num_selected = int(np.sqrt(num_features))\n",
    "            selected_features = np.random.choice(total_features, size=num_selected, replace=False)\n",
    "        elif self.numb_of_features_splitting == \"log\":\n",
    "            num_selected = int(np.log2(num_features))\n",
    "            selected_features = np.random.choice(total_features, size=num_selected, replace=False)\n",
    "        else:\n",
    "            selected_features = total_features\n",
    "        return selected_features\n",
    "\n",
    "    def _find_best_split(self, data: np.array) -> tuple:\n",
    "        \"\"\"Find the best feature and threshold to split data based on minimum entropy.\"\"\"\n",
    "        best_entropy = float('inf')\n",
    "        best_split = None\n",
    "        features_to_check = self._select_features_to_use(data)\n",
    "\n",
    "        for feature_idx in features_to_check:\n",
    "            thresholds = np.percentile(data[:, feature_idx], [25, 50, 75])\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                subset1, subset2 = self._split(data, feature_idx, threshold)\n",
    "                target_values_subset1 = subset1[:, -1]\n",
    "                target_values_subset2 = subset2[:, -1]\n",
    "                subsets = [target_values_subset1, target_values_subset2]\n",
    "\n",
    "                entropy = self._partition_entropy(subsets)\n",
    "                if entropy < best_entropy:\n",
    "                    best_entropy = entropy\n",
    "                    best_split = (subset1, subset2, feature_idx, threshold)\n",
    "\n",
    "        return (*best_split, best_entropy)\n",
    "\n",
    "    def _find_category_probabilities(self, data: np.array) -> np.array:\n",
    "        \"\"\"Calculate probabilities for each category in the data.\"\"\"\n",
    "        categories_as_integers = data[:, -1].astype(int)\n",
    "        total_categories = len(categories_as_integers)\n",
    "        category_probabilities = np.zeros(len(self.categories_in_train), dtype=float)\n",
    "\n",
    "        for i, category in enumerate(self.categories_in_train):\n",
    "            category_index = np.where(categories_as_integers == i)[0]\n",
    "            if len(category_index) > 0:\n",
    "                category_probabilities[i] = len(category_index) / total_categories\n",
    "\n",
    "        return category_probabilities\n",
    "\n",
    "    def _create_tree(self, data: np.array, depth: int = 0) -> TreeNode:\n",
    "        \"\"\"Recursive depth-first tree creation\"\"\"\n",
    "        if depth > self.max_depth:\n",
    "            return None\n",
    "        \n",
    "        subset1, subset2, feature_idx, feature_val, split_entropy = self._find_best_split(data)\n",
    "        category_probs = self._find_category_probabilities(data)\n",
    "        info_gain = self._entropy(category_probs) - split_entropy\n",
    "        node = TreeNode(data, feature_idx, feature_val, category_probs, info_gain)\n",
    "\n",
    "        if min(len(subset1), len(subset2)) < self.min_samples_leaf or info_gain < self.min_information_gain:\n",
    "            return node\n",
    "\n",
    "        node.left = self._create_tree(subset1, depth + 1)\n",
    "        node.right = self._create_tree(subset2, depth + 1)\n",
    "        return node\n",
    "\n",
    "    def _predict_one_sample(self, X: np.array) -> np.array:\n",
    "        \"\"\"Predicts probability for a single sample\"\"\"\n",
    "        node = self.tree\n",
    "        while node:\n",
    "            pred_probs = node.prediction_probs\n",
    "            if X[node.feature_idx] < node.feature_val:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return pred_probs\n",
    "\n",
    "    def train(self, X_train: np.array, Y_train: np.array) -> None:\n",
    "        \"\"\"Trains the model on X and Y datasets\"\"\"\n",
    "        self.categories_in_train = np.unique(Y_train)\n",
    "        train_data = np.hstack((X_train, Y_train[:, None]))\n",
    "        self.tree = self._create_tree(train_data)\n",
    "        \n",
    "        self.feature_importances = {}\n",
    "        for i in range(X_train.shape[1]):\n",
    "            self.feature_importances[i] = 0\n",
    "\n",
    "        self._calculate_feature_importance(self.tree)\n",
    "        total_importance = sum(self.feature_importances.values())\n",
    "\n",
    "        for k, v in self.feature_importances.items():\n",
    "            self.feature_importances[k] = v / total_importance\n",
    "\n",
    "    def predict_proba(self, X_set: np.array) -> np.array:\n",
    "        \"\"\"Returns predicted probabilities for each sample in X_set\"\"\"\n",
    "        pred_probs = np.apply_along_axis(self._predict_one_sample, 1, X_set)\n",
    "        return pred_probs\n",
    "\n",
    "    def predict(self, X_set: np.array) -> np.array:\n",
    "        \"\"\"Predicts labels for each sample in X_set\"\"\"\n",
    "        pred_probs = self.predict_proba(X_set)\n",
    "        preds = np.argmax(pred_probs, axis=1)\n",
    "        return preds\n",
    "\n",
    "    def _print_recursive(self, node: TreeNode, level: int = 0) -> None:\n",
    "        \"\"\"Recursively print the tree structure\"\"\"\n",
    "        if node:\n",
    "            self._print_recursive(node.left, level + 1)\n",
    "            print('    ' * level + '-> ' + node.node_def())\n",
    "            self._print_recursive(node.right, level + 1)\n",
    "\n",
    "    def print_tree(self) -> None:\n",
    "        \"\"\"Prints the tree structure\"\"\"\n",
    "        self._print_recursive(self.tree)\n",
    "\n",
    "    def _calculate_feature_importance(self, node: TreeNode) -> None:\n",
    "        \"\"\"Calculates feature importance by recursively traversing the tree\"\"\"\n",
    "        if node:\n",
    "            self.feature_importances[node.feature_idx] += node.feature_importance\n",
    "            self._calculate_feature_importance(node.left)\n",
    "            self._calculate_feature_importance(node.right)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
